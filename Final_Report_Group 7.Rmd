---
title: "MA 679 Final Report"
author: "Group 7"
date: "2021/5/4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("readxl")
library("dplyr")
library("caret")
library("purrr")
library("randomForest")
library("e1071")
library("pROC")
library("ROSE")
#Data import
data <- read_excel("Health Literacy Transformed Data.xlsx", 
           sheet = "Transformed")

guide <- read_excel("NCCN Guidelines.xlsx")
```

# Introduction

After having some initial exploration of SEER data, we decided to see whether there is a bias between the doctor's decision and the treatment guideline. The treatment guideline is based on the TNM cancer stage, we only have information about the AJCC cancer stage, which means we cannot match all cancer types with their guidelines. Therefore, we only chose cancer on Salivary Gland to do further exploration since its treatment matches well with the data. 


# Data Processing

In the data processing step, we first selected all the Salivary Gland Cancer. In the 'NCCN Guidelines' file, there is no treatment guideline information for stage 'IVC' and 'IVNOS' of cancer Salivary Gland. We removed observations of the 'IVNOS' stage since there are only 6 observations and this amount is trivial compared with the whole dataset. As for the 'IVC' stage, according to the information provided in 'head-and-neck.pdf' page 87, there is no preferred treatment, and the treatment should be individualized based on patient characteristics. Thus, we removed all observations of the 'IVC' stage because we could not decide whether the given therapy follows the guideline or not. Other than these two stages, based on the `NCCN Guidelines`, we found that only stage 'IVB' has a different preferred number one therapy from others, its therapy1 is Radiation while other stages are recommended to have Surgery. Depending on this information, we create a binary column to indicate whether each individual is given treatment following the guideline or not, in this case, '0' means the treatment does not follow the guideline, '1' means the treatment follows the guideline. Meanwhile, we replace all the blank space and symbols in column names for convenience.

Also, we created some new variables based on the EDA:
1. Insurance2: This is a categorical variable with levels 0,1,2. We find that people in the "Insured" class have the lowest rate of being given treatment not following the guideline, while people in the "Uninsured" class have the highest rate of that. "1" indicates the "Insured" class, "2" indicates the "Uninsured" class, and "0" indicates all others.

2. Subsites2: This is a categorical variable with two levels 0 and 1. We find that in the subsite class "C08.1-Sublingual gland", all the observations are given treatment following the guideline. "1" indicates an individual is in this class, "0" indicates all others. 

The specific plots and other results from EDA will be shown in the next part.

Because we decided to use machine learning, we also separate the data into training data and test data and guarantee that the proportion of 0 and 1 responses in both two samples are similar.


```{r}
sali <- data %>% filter(Site == "Salivary Gland")

sali <- sali %>% filter(`AJCC 7 Stage` != "IVC" & `AJCC 7 Stage` != "IVNOS")

sali$follow <- 0

saliB <- sali %>% filter(`AJCC 7 Stage` != "IVB")
unique(saliB$`Surgery Decision`)

for(i in 1:nrow(sali)){
  if (sali[i,]$`AJCC 7 Stage` != "IVB"){
     if(sali[i,]$`Surgery Decision` != "Not recommended"){
       sali[i,]$follow = 1
     }
  } else{
    if(sali[i,]$`Surgery Decision` == "Not recommended" | sali[i,]$`Surgery Decision` == "Not recommended, contraindicated due to              other cond; autopsy only (1973-2002)") {
      sali[i,]$follow == 1
    }
  }
}


sali$Insurance2 <- ifelse(sali$Insurance == "Insured", 1, 0)
for(i in 1:nrow(sali)){
  if(sali$Insurance[i] == "Uninsured"){
    sali$Insurance2[i] <- 2
  }
}

ins2 <- sali %>% filter(Insurance == "Uninsured")

sali$Subsite2 <- ifelse(sali$Subsite == "C08.1-Sublingual gland", 1, 0)
```

```{r}
names(sali) <- gsub("\\s+", "_", names(sali))
names(sali) <- gsub("%_<", "Pct_Less_", names(sali))
names(sali) <- gsub("%_", "Pct_", names(sali))
names(sali) <- gsub("-", "_", names(sali))
names(sali) <- gsub("\\(|)", "", names(sali))
names(sali) <- gsub("\\?", "", names(sali))
```

```{r}
#surgery decision, performed, radiation,  chemo, Mets, subsite, ID.
trainsali <- sali[,-c(1,15,16,20:25)]
trainsali$follow <- as.factor(trainsali$follow)
trainsali$Sex <- as.factor(trainsali$Sex)
trainsali$Year_of_Diagnosis <- as.factor(trainsali$Year_of_Diagnosis)
trainsali$Race <- as.factor(trainsali$Race)
trainsali$Insurance <- as.factor(trainsali$Insurance)
trainsali$AJCC_7_Stage <- as.factor(trainsali$AJCC_7_Stage )
trainsali$SEER_Registry <- as.factor(trainsali$SEER_Registry)

set.seed(123)
getColUni <- function(x){
  length(unique(x))
} 
getSample <- function(data){
  repeat{
    sample <-createDataPartition(y=data$follow,p=0.7,list=FALSE)
    train <- data[sample, ] 
    # length(train$follow)-sum(train$follow==1)
    # test <- data[-sample, ]
    # length(test$follow)-sum(test$follow==1)
    dataCat <- select_if(data,negate(is.numeric))
    trainCat <- select_if(train,negate(is.numeric))
    if(all(rapply(dataCat, getColUni)[-c(1,ncol(dataCat))]== rapply(trainCat, getColUni)[-c(1,ncol(trainCat))])) break
  }
  return(sample)
}

sample <- getSample(trainsali)

train <- trainsali[sample,]
test <- trainsali[-sample,]
```

# EDA
From the bar plots, we observed that this is an imbalanced dataset in terms of Race, Gender, SEER_Registry, and Insurance type.
```{r}
##race
ggplot(data=sali)+geom_histogram(mapping=aes(x=Race,fill=as.factor(follow)),stat='count',position = 'dodge') + theme(axis.text.x = element_text( size = 11, angle = 15))

##sex
ggplot(data=sali)+geom_bar(mapping=aes(x=Sex,fill=as.factor(follow)),stat='count',position = 'dodge')+ theme(axis.text.x = element_text( size = 11, angle = 15))
```


Besides this, itâ€™s notable that there are only 197 observations in class 0 while 2108 observations in class 1, so we also have extremely imbalanced classes in our outcome. 
```{r}
summary(as.factor(sali$follow))
```


From the correlation matrix, we also noticed that, variables like education, including 9th grade,
High school and bachelor are highly correlated. This correlation should caused by the fact that these varibles overlape with eachother. So we also exclude them from modeling.
```{r}
library("PerformanceAnalytics")
data_num <- select_if(sali, is.numeric)  
chart.Correlation(data_num, histogram=TRUE, pch=19)
```



# Modeling

We tried the logistic regression and multilevel logistic regression as our baselines, and then we moved to the classification tree, SVM, and random forest to see if they could return better results.

Since we have very imbalanced classes, we chose not to use the accuracy from the confusion matrix as a standard to compare the model performances because it is misleading in this case. For example, the model could classify half of the 0s wrong, but 99% of 1s correct, but since we have only 197 zeros but 2108 ones in the data frame, the accuracy from the confusion matrix would still be super high. However, we hope our model can perform equally well for each class. What we decided to use for comparing models are: 

1. the confusion matrix result, how many 0s and 1s are misclassified
2. AUC

We used 0.5 as the threshold and classified a probability greater than 0.5 as class1 and others as class0. The AUC results from logistic regression and multilevel regression are 0.72 and 0.754. Random forest with stratified sampling returns an AUC of about 0.8. For all other models, the accuracy of predicting class1 is higher than 90%, but the accuracy of predicting class0 is only about 50% while that of the random forest model with stratified sampling is over 70%.

Based on those two criteria, we decided to use the random forest model for the following analysis.


## Random Forest 

We first fitted a normal randomForest model and tuned the hyperparameters mtry and ntree. We got maximum AUC at 0.725 for mtry = 7 and ntree = 600. 

Next, to deal with the imbalanced classes problem, we tried the stratified sampling method in randomForest with different ratios of class 0s and class 1s. We tuned the partition of the sample size and used 2:3 (60 for class0 and 90 for class1) which returned the maximum AUC. We also tuned mtry and ntree, and the values that returned the largest AUC are mtry = 5 and ntree = 3000.

We drew the variable importance plot using the mean decrease Gini. In both models, AJCC_7_Stage, Age_at_Diagnosis, and Year_of_Diagnosis contribute most to whether the patient followed the guidelines. For the bias that we are considering about, it seems that race, gender, and insurance tend to have a small influence.


```{r tune_rf1, fig.show='hide', eval=FALSE}
# tune: mtry, ntree, sampsize
# create tune grid
mtry <- seq(2, 7, 1)
ntree <- seq(300, 3000, 300)
tune_grid <- expand.grid(mtry=mtry, ntree=ntree)
auc_tune <- c()
# tune mtry, ntree
for(i in 1:nrow(tune_grid)){
  rf_tune <- randomForest(follow ~ Sex + Race + Year_of_Diagnosis + Age_at_Diagnosis + Median_Household_Income + Insurance2 + Lymph_Nodes + Subsite2 + AJCC_7_Stage, data = train, mtry = tune_grid$mtry[i], ntree = tune_grid$ntree[i])
  pred_tune <- predict(rf_tune, test, type = "class")
  auc_tune[i] <- roc.curve(test$follow, factor(pred_tune))$auc
}
mtry_best <- tune_grid[which.max(auc_tune),]$mtry
ntree_best <- tune_grid[which.max(auc_tune),]$ntree
```

```{r rf1}
# From the last chunk, we have the best results of mtry and ntree are:
mtry_best = 7
ntree_best = 600

# Fit random forest
rf = randomForest(follow ~  Sex + Race + Year_of_Diagnosis + Median_Household_Income + Age_at_Diagnosis + Insurance2 + Lymph_Nodes+ Subsite2 + AJCC_7_Stage, data = train, mtry=mtry_best, ntree=ntree_best)
pred_forest = predict(rf, test, type = "class")
confusionMatrix(pred_forest, test$follow)
roc.curve(test$follow, factor(pred_forest))
```

```{r tune_rf2, fig.show='hide', eval=FALSE}  
# create tune grid for random forest with stratified sampling

sampsize_0 <- seq(50,90,10)
sampsize_1 <- seq(50,90,10)
tune2_grid <- expand.grid(mtry=mtry, ntree=ntree, sampsize_0=sampsize_0, sampsize_1=sampsize_1)
auc2_tune <- c()

for(i in 1:nrow(tune2_grid)){
  rf2_tune <- randomForest(follow ~  Sex + Race + Year_of_Diagnosis + Age_at_Diagnosis + Insurance2 + Median_Household_Income + Lymph_Nodes+ Subsite2 + AJCC_7_Stage, data = train, mtry = tune2_grid$mtry[i], ntree = tune2_grid$ntree[i], sampsize = c(tune2_grid$sampsize_0[i], tune2_grid$sampsize_1[i]), strata = train$follow)
  pred2_tune <- predict(rf2_tune, test, type = "class")
  auc2_tune[i] <- roc.curve(test$follow, factor(pred2_tune))$auc
}
mtry2_best <- tune2_grid[which.max(auc2_tune),]$mtry
ntree2_best <- tune2_grid[which.max(auc2_tune),]$ntree
sampsize0_best <- tune2_grid[which.max(auc2_tune),]$sampsize_0
sampsize1_best <- tune2_grid[which.max(auc2_tune),]$sampsize_1
```

```{r rf2}
# from the last chunk, we have the best results of mtry2, ntree2, samplesize0, and samplesize1 are:
mtry2_best = 5
ntree2_best = 3000
sampsize0_best = 60
sampsize1_best = 90


# random forest with stratified sampling
rf2 = randomForest(follow ~ Year_of_Diagnosis + Age_at_Diagnosis + Insurance2 + Median_Household_Income + Lymph_Nodes + Subsite2 + AJCC_7_Stage + Sex + Race, data = train, mtry = mtry2_best, ntree = ntree2_best, sampsize = c(sampsize0_best, sampsize1_best),strata = train$follow)
pred_forest2 = predict(rf2, test, type = "class")
confusionMatrix(pred_forest2, test$follow)
roc.curve(test$follow, factor(pred_forest2))
```


```{r}
varImpPlot(rf,type=2)
```

## Result

To see if there is bias, we compared two models where one of them contains Race and Gender as predictors and the other does not. If we can find an obvious difference in the prediction, then there might be some bias. We still used the confusion matrix result and AUC as standards to compare the model results. 

For the random forest model without stratified sampling method, we found the model without Race and Gender as predictors had a slightly worse prediction, to be specific, this exclusion resulted in the model misclassifying about 4 more observations in class1 to class0, and this causes the AUC decreased around 0.5%. 

For the random forest model with the stratified sampling method, the result is very similar. The model without Race and Gender as predictors misclassified 9 more observations in class1 to class0 compared with the model with Race and Gender, and its AUC is about 2% lower.

From what we got from the results, since there was no obvious difference in the prediction, we cannot conclude the existence of bias in either race or gender. 

```{r}
library(Metrics)
#Results
rf1_2 <- randomForest(follow ~  Year_of_Diagnosis + Median_Household_Income + Age_at_Diagnosis + Insurance2 + Lymph_Nodes+ Subsite2 + AJCC_7_Stage, data = train, mtry=mtry_best, ntree=ntree_best)

pred_forest1_2 <- predict(rf1_2, test)



rf2_2 <- randomForest(follow ~ Year_of_Diagnosis + Age_at_Diagnosis + Insurance2 + Median_Household_Income + Lymph_Nodes + Subsite2 + AJCC_7_Stage, data = train, mtry = mtry2_best, ntree = ntree2_best, sampsize = c(sampsize0_best, sampsize1_best),strata = train$follow)
pred_forest2_2 <- predict(rf2_2,test)


confusionMatrix(pred_forest, test$follow)
confusionMatrix(pred_forest1_2, test$follow)
roc.curve(test$follow, factor(pred_forest))
roc.curve(test$follow, factor(pred_forest1_2))




confusionMatrix(pred_forest2, test$follow)
confusionMatrix(pred_forest2_2, test$follow)
roc.curve(test$follow, factor(pred_forest2))
roc.curve(test$follow, factor(pred_forest2_2))
```
# Discussion

Because we do not have information like TNM stage, so we cannot match treatment guidelines with all head and neck cancer. Therefore, we only choose one site here, and the conclusions we made are all based on that certain site. Once we get to the TNM stage, we can do that on all the data, and it will give us a broader view of bias. 

We do our best to fit a model performing well on both class 1 and class 0, as you can see the model is not perfect due to the limitation of data imbalance. This is a question that often can be seen in machine learning since in the real world those imbalance things always happen. Our model here is only a reference to give a sense of what and how this bias will happen, and due to the accuracy of the model, we cannot assert that it must or it must not have bias.
